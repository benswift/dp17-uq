\documentclass[a4paper,fontsize=12pt]{scrartcl}
\usepackage{Alegreya}
\usepackage{AlegreyaSans}

\usepackage[svgnames,hyperref]{xcolor}
\usepackage{url}
\usepackage{graphicx}

% \usepackage{fullpage}

\usepackage[%
backend=biber,
style=authoryear, % numeric-comp
maxbibnames=10,
url=false,
doi=false]{biblatex}

% \AtEveryBibitem{\clearfield{month}}
% \AtEveryCitekey{\clearfield{month}}

\addbibresource{references.bib}

\usepackage[english=british,threshold=15,thresholdtype=words]{csquotes}
\SetCiteCommand{\parencite}

\newenvironment*{smallquote}
{\quote\small}
{\endquote}
\SetBlockEnvironment{smallquote}

\usepackage[%
unicode=true,
hyperindex=true,
bookmarks=true,
colorlinks=true, % change to false for final
pdfborder=0,
allcolors=DarkBlue,
% plainpages=false,
pdfpagelabels,
hyperfootnotes=true]{hyperref}

\author{}

\date{\today}

\begin{document}

\subsection*{Overall comments}
\label{sec:overall-comments}

\noindent
\textbf{Note to readers:} comments/edits very welcome. Some general
thoughts and questions:
\begin{enumerate}
\item how specific should we be with examples? is it a ``general
  problem'' which we're solving, and the examples (e.g. flood/tsunami
  modelling, bushfire etc.) are just exemplars? Or do we want to pick
  1--3 problem domains which we want to tackle, and major on the
  concrete contributions we can make there?
\item feel free to correct any funny maths notation, I'm not
  necessarily up on what the greek letters \emph{du jour} are.
\item How do we balance the emphasis on UQ with the problem of ``model
  accessibility'' more generally? How do we most effectively weave the
  UQ stuff into the description of the problem? Examples are crucial here.
\end{enumerate}

\renewcommand{\thesection}{\Alph{section}}

\subsubsection*{PROJECT TITLE}

\setcounter{section}{3} % C1.
\subsection{Project Description}
\label{sec:project-description}
% (Please upload a Project Description as detailed in the Instructions
% to Applicants in no more than 10 A4 pages and in the required
% format. Please refer to the Instructions to Applicants for detailed
% instructions on the required content and format of the Project
% Description.)

\emph{need to be careful with vocab here---modelling/simulation,
  quantities/features, etc.}

\subsubsection*{AIMS AND BACKGROUND}
% - Describe the aims and background of the Proposal
% - Include information about national/international progress in this
%   field of research and its relationship to this Proposal
% - Refer only to outputs that are accessible to the national and international research communities.

Modelling is the process of exploring and predicting the relationship
between quantities, the nature and meaning of which differs between
application domains:
\begin{itemize}
\item in a manufacturing process we may be interested in the
  relationship between the quality of the input materials and the
  defect rate in the finished widgets
\item in design/engineering, we might be interested in the
  relationship between an airfoil's shape (at both micro and macro
  scales) and its aerodynamic properties
\item in urban planning, the relationship between road and rail
  topologies and traffic congestion
\item in climate science, the relationship between atmospheric CO2
  levels and global temperatures and weather
\item in disaster response scenarios, predicting future impacts of
  fire/flood threats based on current sensor readings and reports from
  the field
\item in economic policy, the relationship between fiscal policy
  settings and other features of the macroeconomy
\end{itemize}

In these situations, there are some quantities we \emph{know}
(including some parameters we can control) and some quantities we
\emph{want} to know---our model inputs and outputs
respectively. More formally, we have a model
\begin{equation}
  \label{eq:1}
  u_{\mathbf{p}}(\mathbf{x})
\end{equation}
defined for $\mathbf{x}$ in some domain, with a (set of) parameters
$\mathbf{p} \in \mathcal{P}$. One can think of $\mathbf{x}$ as the input
parameters intrinsic to the problem domain (e.g. spatial dimensions,
sensor readings or economic indicators) while the model parameters $\mathbf{p}$
are ``knobs to tweak'' in the model.

In many (most?) cases the goal is to better understand the
relationship between $\mathbf{p}$ and some lower-dimensional (simpler?)
quantity of interest $Q(u_{\mathbf{p}})$, or to find the parameter choice $\mathbf{p}$
which optimises $Q$ over all $\mathbf{x}$. It is worth pointing out
that in the abstract, this process does not require formal
computational modelling at all, in fact in many real-world situations
the optimal settings (i.e. the optimal $\mathbf{p}$) for a given process are
obtained through trial and error, expert judgements or semi-educated
guesswork. There may be many reasons for this---perhaps there are no
appropriate computational models for the domain, perhaps the models
exist but are cumbersome to use/configure, or perhaps the ultimate
decisionmakers are unconvinced of the usefulness of computational
modelling vs the ``good old ways'' of reading the tea leaves. Whatever
the reason, the key point is that modelling, when considered
holistically, is a complex socio-technical system with feedback loops
involving both machine and human intelligence.

When considered as a human factors problem, the challenge is to provide
the decision-maker (who may or may not understand the subtleties of
the model) with tools and interfaces for \emph{exploring} the
relationship between $\mathbf{p}$ and $Q(u_{\mathbf{p}})$. This will allow them to better
understand the connection (and the associated uncertainty) between
these different dimensions of $\mathbf{p}$ (i.e. the knobs under their control)
and the overall response of the system. Otherwise, they may choose
suboptimal parameters, or at worst mistrust and ignore the results of
the modelling altogether.

\emph{Should we formally mention UQ here? Perhaps through an example}

\subsubsection*{RESEARCH PROJECT}
% - Describe how the research is significant and how it addresses an important problem
% - Describe how the Proposal meets the objectives of the Discovery Projects scheme
% - Describe how the anticipated outcomes will advance the knowledge base of the discipline and
%   how the Proposal aims and concepts are novel and innovative
% - Outline the conceptual framework, design and methods and demonstrate that these are
%   adequately developed, well integrated and appropriate to the aims of the Proposal. Include
%   research plans and proposed timelines
% - Detail what new methodologies or technologies will be developed in the course of the research
%   and how they will advance knowledge
% - Outline the feasibility of the project, in terms of design, budget and proposed timeline
% - If the rationale for some of the Proposal rests upon manuscripts that are still in the process of
%   being published, or on results of work that may not be available to assessors, include a
%   summary of the relevant work
% - Describe the expected outcomes and likely impact of the proposed research
% - Describe how the Proposal might result in national or international economic, commercial,
%   environmental, social and/or cultural benefits
% - If the research has been nominated as focussing upon a topic or outcome that falls within one
%   of the Science and Research Priorities, describe the potential for the project to contribute to the associated Priority Goals.

To make models more accessible and \emph{useful} to decisionmakers, we
will develop a framework for building interactive
interfaces/environments for model exploration, especially uncertainty
quantification/sensitivity analysis.

There are two main challenges in designing such an interface:
\begin{enumerate}
\item how do we perform the modelling step $\mathbf{p} \rightarrow
  u_{\mathbf{p}}(\mathbf{x}) \rightarrow Q(u_{\mathbf{p}})$ sufficiently quickly
  for interactive exploration, while still providing adequate
  fidelity?
\item how do we present the quantities $\mathbf{p}$, $u_{\mathbf{p}}(\mathbf{x})$
  and $Q(u_{\mathbf{p}})$ to the decision-maker (and take interactive input) in a
  way which provides meaningful insight into the validity of the
  model, ultimately providing actionable knowledge?
\end{enumerate}

\paragraph{Efficient parameter space exploration with sparse grids}

\emph{This is Brendan's stuff. At some stage I'll make sure it flows
  nicely.}

With mathematical models becoming increasingly complex one finds that
the parameter spaces which are inputs to such models are high
dimensional. This makes problems like parameter estimation and
uncertainty quantification more and more difficult. There has been
much interest in reduced basis methods to make such problems more
tractible [references]. Sparse grid methods have also proved to be
highly successful in the study and computation of high dimensional
problems [many references].

Another challenge faced by scientists is the ever increasing number of
algorithms one has available to approximate the solution of a variety
of mathematical models. Scientists frequently ask the questions: which
model/method should I use, and what is the best model/method for my
problem? Recent work on sparse grid techniques is finding new ways to
combine results from not only mathematical models computed via
different methods [sparse grid extrapolation techniques] but also
entirely different mathematical models [Jouke et.al. work on ship hull
with 2 models]. It is clear that different methods and models have
their own advantages and disadvantages which have the potential to be
combined in new and interesting ways. [Point trying to make: a) the
usual 'too many methods' dilemma, and b) there is generally no one
'best' method and yet it is typical to pick one and stick with it
(based on some comparison with others)]

Of course the difficulty is that such exploration is time consuming
and can be difficult to keep track of. By combining all of these
methodologies and techniques into a single framework one may more
easily test and combine the vast number of different approaches to
solving any given problem. Further, by implementing this within a live
programming environment, users can quickly adjust methods and
parameters as needed whilst getting near instant feedback and
response. Such tools could significantly increase the speed at which
research is undertaken and allow us to learn more from our
mathematical models than previously envisioned.

This section will also include the offline/online stuff, and the
flexible cloud infrastructure stuff as well.

\paragraph{Navigating complex data with immersive environments}

\noindent
\textbf{TODO Henry} do you have some stuff from your last grant which
we can use here?

Why is interactivity important? Because it takes the abstract
connection between $\mathbf{p}$ and $Q(u_{\mathbf{p}})$ and makes it a malleable,
responsive, tactile one. What happens when I change $\mathbf{p}$ a little bit
in this direction? What about in \emph{that} one? How about \emph{both
  together?}


\subsubsection*{ROLE OF PERSONNEL}
% - Summarise the role, responsibilities and contributions of each Chief Investigator (CI) and Partner Investigator (PI)
% - Summarise the roles and levels of involvement of other Participants, for example, technical staff, Research Associates and other personnel
% - Describe how each Participant will ensure they have the ‘time and capacity’ to undertake the proposed research, taking into account any other grants or roles that they hold.



\subsubsection*{RESEARCH ENVIRONMENT}
% - Outline the adequacy and opportunities within the Research Environment in your relevant department, school or research group, and the extent to which it will provide for knowledge growth, innovation, collaboration, mentoring and student training
% - Describe the existing, or developing, research environment within the Administering Organisation and collaborating Organisation(s) which will enable this Project
% - Describe how the Project aligns with the Administering Organisation’s research plans and strategies.



\subsubsection*{COMMUNICATION OF RESULTS}
% - Outline plans for communicating the research results to other researchers and the broader community, including but not limited to scholarly and public communication and dissemination.



\subsubsection*{MANAGEMENT OF DATA}
% - Outline plans for the management of data produced as a result of the proposed research, including but not limited to storage, access and re-use arrangements.
% - It is not sufficient to state that the organisation has a data management policy. Researchers are encouraged to highlight specific plans for the management of their research data.



\subsubsection*{REFERENCES}



% guff BEGINS HERE

lawrence murray (CSIRO)

malcolm sambridge (ANU)

domain of applicability

noninvasive!

GAs as alternatives (Zoe Brain)

point to problems of the size where they will be advantageous (d ~ 10)

if the goal is optimization, talk about GAs

sensitivity analysis

talk about all the competing approaches (MC, autodiff, non-autodiff)

go away and develop a simple proof-of-concept (be careful that you
don't attract this criticism)










With mathematical models becoming increasingly complex one finds that
the parameter spaces which are inputs to such models are high
dimensional. This makes problems like parameter estimation and
uncertainty quantification more and more difficult. There has been
much interest in reduced basis methods to make such problems more
tractible [references]. Sparse grid methods have also proved to be
highly successful in the study and computation of high dimensional
problems [many references].

Another challenge faced by scientists is the ever increasing number of
algorithms one has available to approximate the solution of a variety
of mathematical models. Scientists frequently ask the questions: which
model/method should I use, and what is the best model/method for my
problem? Recent work on sparse grid techniques is finding new ways to
combine results from not only mathematical models computed via
different methods [sparse grid extrapolation techniques] but also
entirely different mathematical models [Jouke et.al. work on ship hull
with 2 models]. It is clear that different methods and models have
their own advantages and disadvantages which have the potential to be
combined in new and interesting ways. [Point trying to make: a) the
usual 'too many methods' dilemma, and b) there is generally no one
'best' method and yet it is typical to pick one and stick with it
(based on some comparison with others)]

Of course the difficulty is that such exploration is time consuming
and can be difficult to keep track of. By combining all of these
methodologies and techniques into a single framework one may more
easily test and combine the vast number of different approaches to
solving any given problem. Further, by implementing this within a live
programming environment, users can quickly adjust methods and
parameters as needed whilst getting near instant feedback and
response. Such tools could significantly increase the speed at which
research is undertaken and allow us to learn more from our
mathematical models than previously envisioned. [This is somewhat
genereic/non-specific at the moment]


\subsubsection*{PROJECT TITLE}

\textbf{Human-in-the-loop Model Uncertainty Quantification for
  Time-Critical Decisionmaking (TBC)}

Inverse uncertainty quantification is a critical aspect of any
simulation/modelling workflow. As numerical modelling becomes
increasingly pervasive---from industrial design to basic science to
economic policymaking---the ability to understand and update these
models in response to measurements from the real world is critical in
translating insights from modelling and simulation to actionable
knowledge. A rich understanding of the uncertainty in a model is also
important when the model is used as an input to a human decisionmaking
process (as most models invariably are). What can we say about the
strength of the connection of the different aspects of the model to
reality? How confident can we be about various predictions given by
the model?

In addition, recent developments in sensor networks and pervasive
monitoring technologies have increased our ability to gather data from
the world. Streaming data analysis is an active area of research, and
in an uncertainty quantification context streaming data provides the
opportunity to be continuously estimating model uncertainties and
updating our models in response to new data coming in.

This project asks the question: \textbf{can streaming data be used to
  update and model uncertainties and provide intelligible insights in
  time-critical decisionmaking scenarios?} (TODO this needs to be
tighter)

One example scenario: sensors detect an earthquake in the Pacific Ocean,
and the countdown timer begins---what is the likelihood of a
destructive tsunami, where will it impact, and does an evacuation
order need to be issued? Some initial modelling has already been done,
but new readings are coming in regularly from sensor buoys throughout
the ocean. How can this new (and potentially noisy) data be used to
update the model, and how does this affect the predicted impact? How
does uncertainty in the buoy readings affect our confidence in the
model?

This problem has two main components: a mathematical component, and a
a human factors component.
\begin{itemize}
\item We will deal with the \textbf{mathematical challenges} using a
  sparse grids approach, using combination techniques
  to incorporate new measurements/information in an efficient way.
\item We will deal with the \textbf{human factors challenges} using a
  live steering (maybe even live programming?) approach, building
  interfaces which provide expert decisionmakers with the ability to
  steer computation/optimisation procedures, exploring different model
  parameterisations and scenarios with real-time feedback.
\end{itemize}

\subsubsection*{AIMS AND BACKGROUND}

\paragraph*{Aims}\mbox{}\\

The modern world appears to be saturated with computing power.
Contemporary smart phones outperform the original Cray~1
``supercomputer'' by three orders of magnitude. Accessing contemporary
high-performance computing (HPC) resources has never been easier and,
when combined with machine learning\cite{Hastie2009}, and big
data\cite{Manyika2011}, modern supercomputing performance has the
potential to provide answers to deep questions in science, business
and the humanities.

Modern comodity computing devices are spectacularly \textbf{visual}
and \textbf{interactive}, enabling users to slide, tap, watch, speak
and listen with real-time feedback. It is this combination of
interactivity with power which makes mobile and personal computing
useful, usable and an agent of change.

But the closer one comes to true high-performance computing, the less
interactive computing systems become. Due to a combination of
technical and social factors, high-performance scientific computing
(HPSC) is still often performed in a batch \emph{compile-run-analyse}
style: simulation codes are complex both conceptually (i.e.~the
underlying mathematics and physics) and in implementation (distributed
computation, large codebases). Supercomputing resources are expensive
and need to be kept secure.

It is worthwhile imagining some example scenarios that might be
possible if better interactive \textbf{access to} and \textbf{steering
  of} scientific supercomputing applications were to be available:
\begin{enumerate}
\item A computational physicist submits a job to a supercomputer which
  takes 3 weeks to execute. But subsequent monitoring of incremental
  visualisations indicates that something about the simulation does
  not make sense.A check of the, almost 200, input parameters reveals
  that two of these have been incorrectly set. The phyicist cancels
  this run and submits another.

\item That same physicist has a sudden insight into the physical
  system being modelled as a result of watching some of the
  incremental visualisations of the running code. She initiaties a new
  simulation with several changes to the parameter deck before the
  first simulation is complete. Both simulations turn out to be useful
  in understanding the phenomenon under study.

\item Our physicist decides to improve the numerical accuracy of the
  model itself. She quickly modifies one of the functions being called
  by that simulation, one that implements flux-conservation across
  computational elements of the magneto-fluid being modelled. She then
  tests her new function on the active program by branching the
  simulation and swapping the function call on active data. Through a
  comparison of the two active simulations, she is able to estimate
  what the effects of her new numerical method will be on the
  accumulation of errors in the simulation data. She lets both
  simulations run to completion, and in her subsequent data analysis,
  she is able to use her error estimates to adjust and compare
  sumulations using both methods.

\item Performance diagnostics reveal that a multicore simulation is
  very unbalanced in terms of processor allocation. Due to the
  complexity of this simulation, the only way to rebalance resources
  is to rewrite part of the code which maps the geometric grid to the
  processor model. Our physicist is able to rewrite this function and
  to swap the new version in, in real-time. The simulation continues
  seemlessly with enhanced use of resources and finishes faster.

\item A computer scientist is developing a custom visualisation of a
  scientific simulation. Because of the size of the dataset being
  rendered, and because of its distribution across hundreds of
  processors it is impossible to move all of the data. Instead each
  data nodes needs to be ``visited'' and a sample of that data
  transferred for visualisation. This visualisation is planned, tuned
  and debugged on running code and is complete by the time actual the
  simulation is finished.

 % \item A bioinformatics researcher runs a supercomputer simulation which searches and matches millions of generated phylogenic candidates in order to develop evolutionary trees. Rather than 

%A subsequent analysis of the results finds that the stopping
%   criteria were satisfied after only the first hundered such files had
%   been compared.

\item A team of emergency-services personnel are monitoring a
  real-time satellite feed of a bushfire which is hours away from
  threatening homes and lives. A computational simulation of the
  bushfire is being monitored and compared with data but it appears to
  be giving unreliable results. The simulation experts quickly
  initiate several companion simulations having different values of
  fuel loads. The fire-front trajectories of these companion
  simulations neatly bracket those of the main simulation and provide
  an estimate of the confidence that can be ascribed to those
  predicted fire-fronts.


%\item A growing hi-tech manufacturing company has historically relied
%  on managing their supply chain by hand. However, as the company
%  grows and the number of supplier relationships increases this is no
% longer feasible. A purely automated approach generates suboptimal
%  results, because the supply chain involves a complex network of
%  human relationships and hand-shake agreements between the suppliers.
%  The expert team can see subtle problems with the results (and
%  appropriate subtle corrections) as the automated model is running,
%  but are prevented by the software from making these changes until
%  the model has finished running.
\end{enumerate}

All of these scenarios benefit from a tight human-in-the-loop (HIL)
coupling between run-time analysis, visualisation and high-performance
simulation software. The benefits include greater \emph{efficiency}
and greater \textbf{potential for discovery} in scientific workflows.
\textbf{The aim of this project proposal is thus to build
  human-in-the-loop interactivity into high performance scientific
  computing}.

\paragraph*{Background}\mbox{}\\

In 2007, a National Science Foundation (NSF)
workshop\parencite{Gil2007} highlighted the need for dynamic
interactivity as a looming ``grand challenge'' in scientific
computation and data analysis.
% Other challenges included data provenance (where data comes
% from\cite{McPhillips2009}), reproducibility and shareability of
% complex data processing software pipelines.
In spite of many advances in the field of computational steering it is
clear that challenges remain. A recent (2014) review, ``Dynamic
steering of HPC scientific workflows: A survey''\parencite{Mattoso},
identified six key areas of interaction/steering:
\begin{enumerate}
\item \textbf{monitoring}: the ability to monitor intermediate
  quantities of interest (e.g.~total energy)
\item \textbf{analysis}: the ability to perform analysis of simulation
  quantities (e.g.~viewing a live histogram of the energy
  distribution)
\item \textbf{adaptation}: the ability to adapt/modify the running
  simulation
\item \textbf{notification}: the ability of the system to alert users of the
  need/opportunity to interfere
\item \textbf{interface for interaction}: the interface (either command-based
  or GUI) presented to the user to monitor and adapt the computation
\item \textbf{computing model}: the ability to tailor the types of interaction
  depending on the compute model in use (e.g.~providing full
  monitoring when running locally on a workstation vs only statistical
  sampling when the job is distributed over a cluster)
\end{enumerate}
At the end of their survey, Mattoso et~al.~particularly identify the
\emph{analysis} and \emph{adaptation} branches of their taxonomy as
not being well addressed by the current state of the art.

Elaborating further, in terms of \emph{real-time analysis} the open
challenges are:
\begin{itemize}
\item \textbf{in-situ analysis}: modern terascale compute
  infrastructure is often IO-bound, and HPC applications must minimise
  inter-node communication to achieve efficient use of the hardware.
  As a result, it is often unfeasible to move the data of interest
  back to a central node for processing and analysis, so any such
  analysis must be performed in-situ\cite{Bennett2012} on the compute
  node itself.
\item \textbf{decision-support tools}: giving a user the
  \emph{ability} to interfere with a running computational process is
  one thing---knowing \emph{when and how} to interfere is another
  thing altogether. Dynamic SWfMS must provide as useful real-time
  feedback to the user (a domain expert) to assist them in deciding
  when and how to interfere. This problem is an example of a more
  general end-user programming (EUP)\cite{Myers2006} problem---giving
  domain experts the appropriate tools to transfer their domain
  expertise into the computational domain.
\end{itemize}
In the area of \emph{adaptation} the open challenges are:
\begin{itemize}
\item \textbf{dynamic workflow engines}: most of the surveyed systems
  allowed the user to adapt the computation at a high level (e.g.
  add/delete subtasks). However, there was much less support for
  modifying the evolution of specific tasks (e.g.~swapping out
  low-level algorithms and tight-loops in computation). This level of
  adaptivity would allow for even more steering possibilities, but may
  exacerbate the problem of deciding when/how to interfere (although
  support for rolling back interventions would be helpful here).
\item \textbf{parameter slice exploration}: many scientific problems
  involve searching high-dimensional parameter spaces for optimal
  parameter configurations, and many algorithms exist for this task.
  However, there are some problems (e.g.~buisness informatics) where
  cost/loss functions are hard to articulate, and more traditional
  purely numerical optimization approaches tend to overfit or get
  stuck in local maxima. In these situations an expert domain user, if
  presented with appropriate feedback in real-time, may be able to
  recognise these type of convergence problems early on, and perform
  appropriate algorithmic and parameter adaptations to avoid the
  problem.
\end{itemize}

The authors of this project proposal contend that all of these
challenges can now be effectively investigated by deploying recent
developments in programming language systems, known as ``live
programming'' (LP), into the context of high performance scientific
simulation. Live programming is a term which has been used to denote
systems which support the direct intervention of the programmer in a
program's runtime state. It can be thought of as an extreme version of
just-in-time (JIT) programming where there is a direct correlation
between a program's representation and its execution. An example of
this is the visual language Self\cite{Ungar1987} where each visual
element in the language is a directly programmable object. As the
ambition of LP systems has grown, these systems and languages
themselves have evolved to have access to clock-time and other
computer-system properties at a language level in order to, for
example, create, modify and interact with computer music and devices
in real time. Examples of the latter include the
Impromptu\cite{Sorensen2005} and Extempore\cite{Sorensen} languages
which one of the authors of this proposal has been working on and with
over several years.


\subsubsection*{RESEARCH PROJECT}

% explore $\rightarrow$ enliven $\rightarrow$ empower

% Want to bring the idea of ``liveness'' to HPC/big data software – allow
% developers to get rapid, incremental update of target data process
% algs ; allow developers to get near-instantaneous feedback on input of
% changes to outputs data via rich visualisations

% Want to leverage domain-specific visual languages to specify these big
% data analysis problems to enable end users a high-level, controlled
% environment c.f. R-type scripting, code, BI configs etc

% Want to leverage rich information visualisations to provide small
% snapshot and overall output understanding by end users

% << Worth a big picture ``concept diagram'' here >>

% The aim of this project is thus to invent a new approach to specifying
% and realizing data-centric software systems using live visual
% programming and data visualisation we break into specific aims:

% - SDL++ for data analytics
% - Live visual programming of data analytics - DSVLs
% - extempore -> live update of GPU/HPC/IR/ML algs etc
% - live visualisations of complex data analytics alg data for feedback,
%   output
% - evaluate effectiveness by apply to several exemplars e.g.~particles,
%   big data analysis, infoVis...

This project will allow domain experts to better harness the power of
computational simulation the in a safe and effective way by creating
tools and interfaces for \emph{interactive} computational simulation
and anaylsis. This involves integrating research and tools from the
domains where such computational analyses are used with insights from
human-computer interaction and software workflows.

Through a series of exemplar case studies in different domains, we
will unlock new ways to perform complex computation analyses through
three steps:

\begin{enumerate}
\item \textbf{explore}: first, by exploring the problem domain (and
  existing tools) of interest in conjunction with domain experts, do
  determine the dimensions of the domain which could most benefit from
  interactive, human-in-the-loop analysis
\item \textbf{enliven}: taking these modelling and simulation codes
  and providing new interfaces for real-time interaction---this is not
  just a matter of superficially wrapping these systems in a slick
  GUI, but requires a restructuring of codes which are designed with
  batch processing in mind to allow for meaningful interaction and
  feedback
\item \textbf{empower}: using a user-centred design approach, we will
  create appropriate interfaces (e.g.~graphical user interfaces) to
  empower these domain experts to unlock their expertise in
  interactive computational simulation
\end{enumerate}

\paragraph*{Research significance}\mbox{}\\


Computational simulation and modelling is ubiquitous in both academia
and industry, and its importance is growing. Large-scale basic physics
relies on computational simulation to probe the subatomic landscape;
the auto industry uses simulation extensively for calculating the
efficiency of new designs; and algorithmic pattern recognition and
recommendation has transformed the way we search (Google), shop
(Amazon), watch (Netflix).

However, performing these analyses increasingly requires specialised
computational skills such as high-performance and distributed
computing. There is a risk that the power of simulation is available
only to a HPC-literate elite, and to those rare experts who assimilate
both the domain knowledge and computing skills to access it, and to
interpret it's results.

This research is significant because it aims to unlock the power of
computational simulation for interactive use. Although we concentrate
our research on case studies from extant computational science, the
ultimate potential of this work is to eventually empower domain
experts from a broad range of areas to use the high-performance
computing power which is now available to them. We envision a future
where performing a complex particle physics simulation is as
interactive and \emph{alive} as flicking through photos on an iPad,
rather than a tedious ``queue up the job, come back a week later to
pore over a dump of the results'' process. This benefits not only
experts already working in that field, but also allows new researchers
(or even schoolchildren) to understand the behaviour of such systems
through interactive play rather than dry textbooks.

% Big data software, HPC software becoming massively important
% Development of both still rudimentary, limited use of ``liveness''
% concept in HPC/DSVLs to date (some in InfoVis). Many applications of
% our approach; a very enabling technology. Examples might include\ldots

\paragraph*{Advancing the knowledge base with innovation}\mbox{}\\

As highlighted by Mattoso et~al.\parencite{Mattoso}, there is a desire for
more dynamic interactivity in scientific workflows, but progress in
that area has been limited. This project will contribute to the
knowledge base by providing these dynamic workflows, allowing us to
answer open questions such as:

\begin{itemize}
\item how can interactivity most effectively be ``added'' to existing
  scientific workflows?
\item how can automated assistance/analysis be provided to the user to
  best guide their interaction?
\item how do other outstanding issues in scientific computation, such
  as data provenance, reproducibility, etc.~impact the interactive
  workflow?
\end{itemize}

% \begin{itemize}
% \item very limited use of ``liveness'' concept to date in BI, HPC, etc;
%   adding into these very novel
% \item limited DSVLs leveraged to date in these domains; enables end
%   users
% \item InfoVis well studied but specification low-level, limited;
%   enables greater generality
% \item running the concept of ``liveness'' through these: DSVLs <->
%   HPC/Big Data code <-> InfoVis
% \end{itemize}

\paragraph*{Conceptual}\mbox{}\\

Series of research problems to be solved:

\begin{enumerate}
\item Add ability to modify HPC/Big Data code while
  running---``liveness''
\item Invent set of DSVLs for specifying such applications
\item Use DSVLs to generate code / modify code (via 2, 1 above)
\item Specifying rich, live InfoVis solution for data processed (by 1
  above)
\item Evaluate effectivess on set of exemplars
\end{enumerate}

\paragraph*{Feasibility, methodology and timeline}\mbox{}\\

% -set of steps
% -set of prototype interations

% Work per quarter

\paragraph*{Outcomes and impact}\mbox{}\\

Real, working software workflows (open-source!)

\paragraph*{National benefit and strategic research priorities}\mbox{}\\



\subsubsection*{ROLE OF PERSONNEL}



\subsubsection*{RESEARCH ENVIRONMENT}

\paragraph*{Existing research environment}\mbox{}\\

ANU is a great place to do this.

\paragraph*{Developing research environment}\mbox{}\\

Need to build the team.

\paragraph*{Collaboration, communication and commercialization}\mbox{}\\

\subsubsection*{COMMUNICATION OF RESULTS}

reproducable code/software artefacts

\subsubsection*{MANAGEMENT OF DATA}

Data provenance, reproducibility

\subsubsection*{REFERENCES}

\vskip -2em % filthy hack to get the spacing right

\printbibliography[title=\ ]

\newpage
\setcounter{section}{5} % E1.
\setcounter{subsection}{0}
\subsection{Justification of funding requested from the ARC for the
  duration of the Project}
\label{sec:funding-justification}
% (In no more than 5 A4 pages and within the required format fully
% justify, in terms of need and cost, each budget item requested from
% the ARC. Use the same headings as in the Description column in the
% budget at Part D of this Proposal. )



\newpage
% E2.
\subsection{Details of non-ARC contributions}
\label{sec:non-arc-contributions}
% (In no more than 2 A4 pages and within the required format, provide
% an explanation of how non-ARC contributions will support the
% proposed Project. Use the same headings as in the Description column
% in the budget at Part D of this Proposal. )


\newpage
\setcounter{section}{6}
\setcounter{subsection}{11} % F12. 
\subsection{Research Opportunity and Performance Evidence (ROPE)\\
  Recent significant research outputs and ARC grants (since 2005)}
\label{sec:recent-significant-outputs}
% (Please attach a PDF with a list of your recent significant research
% outputs and ARC grants most relevant to this Proposal (20 pages
% maximum). Please refer to the Instructions to Applicants for the
% required content and formatting.)



\newpage
% F13. 
\subsection{Research Opportunity and Performance Evidence (ROPE)\\
  Ten career-best research outputs}
\label{sec:ten-best-outputs}
% (Please attach a PDF with a list of your 10 career-best research
% outputs, with a brief paragraph for each output explaining its
% significance (5 pages maximum). Please refer to the Instructions to
% Applicants for the required content and formatting.)



\newpage
\setcounter{section}{7} % G1. 
\setcounter{subsection}{0}
\subsection{Research support from sources other than the ARC}
\label{sec:other-research-support}
% (For all participants on this Proposal, please provide details of
% research funding from sources other than the ARC (in Australia and
% overseas) for the years 2014 to 2018 inclusive. That is, list all
% Projects/Proposals/Fellowships awarded or requests submitted for
% funding involving the Proposal participants. Please refer to the
% Instructions to Applicants for submission requirements.)

That's all I've got. Merry Christmas!

\section{TODO}
\label{sec:todo}

\begin{itemize}
\item references
\item example projects
\item all the rest of it\ldots
\end{itemize}

% \section{Conclusion}
% \label{sec:conclusion}

% Please give us all the money, and we'll do amazing science.

\printbibliography[title=References]

\end{document}

% Local Variables:
% TeX-engine: xetex
% End:

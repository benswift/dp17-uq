\documentclass[a4paper,fontsize=12pt]{scrartcl}
\usepackage{Alegreya}
\usepackage{AlegreyaSans}

\usepackage[svgnames,hyperref]{xcolor}
\usepackage{url}
\usepackage{graphicx}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage[utf8]{inputenc}

% \usepackage{fullpage}

\usepackage[%
backend=biber,
style=authoryear, % numeric-comp
maxbibnames=10,
url=false,
doi=false]{biblatex}

% make bibliography a bit smaller if necessary
\renewcommand*{\bibfont}{\small}

% \AtEveryBibitem{\clearfield{month}}
% \AtEveryCitekey{\clearfield{month}}

\addbibresource{references.bib}

\usepackage[english=british,threshold=15,thresholdtype=words]{csquotes}
\SetCiteCommand{\parencite}

\newenvironment*{smallquote}
{\quote\small}
{\endquote}
\SetBlockEnvironment{smallquote}

\usepackage[%
unicode=true,
hyperindex=true,
bookmarks=true,
colorlinks=true, % change to false for final
pdfborder=0,
allcolors=DarkBlue,
% plainpages=false,
pdfpagelabels,
hyperfootnotes=true]{hyperref}

\usepackage{todonotes}

\author{}

\date{\today}

\begin{document}

\renewcommand{\thesection}{\Alph{section}}

\setcounter{section}{3} % C1.
\subsection{Project Description}
\label{sec:project-description}
% (Please upload a Project Description as detailed in the Instructions
% to Applicants in no more than 10 A4 pages and in the required
% format. Please refer to the Instructions to Applicants for detailed
% instructions on the required content and format of the Project
% Description.)

\subsubsection*{PROJECT TITLE}

\textbf{Live and interactive parameter-space exploration of environmental simulations with uncertainty quantification} %can differ from grant title and be longer 



\subsubsection*{AIMS AND BACKGROUND}

% - Describe the aims and background of the Proposal
% - Include information about national/international progress in this
%   field of research and its relationship to this Proposal
% - Refer only to outputs that are accessible to the national and international research communities.

% \begin{itemize}
% \item \textbf{Goal:} understanding uncertainty/risk relationships in models
% \item \textbf{Context/domain:} fire \& flood
% \item \textbf{Method:} interactive audiovisual interfaces for
%   manipulating model parameters in real-time
% \item \textbf{``Secret sauce'':} smart \& cheap approximations to expensive
%   models using sparse grids and reduced basis models
% \end{itemize}

When systems are disrupted during environmental disasters (for example,
flood, fire, wind, earthquakes) information from
computational simulation can be needed urgently and in an
acceptable format to aid  decision-making by emergency services. 
Meeting such a modelling and simulation 
challenge is not simply a matter of compute power but involves
ideas from software engineering, computational mathematics,
human computer interaction, high performance computing, and data visualisation. Furthermore, all
of these disciplines need to be tightly coupled and benchmarked to
realistic human-in-the-loop scenarios involving emergency services personal in collaboration with modelling experts. 
In particular, 
a crucial aspect of advice provided from modellers to decision makers is to properly quantify and communicate 
the {\em uncertainty} in predictions obtained
by computer simulations.

We address this
challenge through a {\em live-programming} approach to software system development
and deployment of {\em live and interactive} environmental simulations. A particular focus will be on 
 {\em  quantifying the 
uncertainty} of the core environmental simulations being used and in presenting this uncertainty to decision makers.
We will deploy  ``live"  distributed
computing infrastructure to {\em rapidly prototype} modelling software and to build systems which effectively transform the
traditional, batch-oriented modality of environmental simulation into a highly-interactive and run-time modifiable one.
Our vision is that decisionmakers
will be able to examine many different environmental disaster scenarios in a short space of
time, even with computationally-expensive models. This will enable
them to develop an intuitive understanding of the uncertainty
relationships in the system---from uncertainties in input data through
to representations of uncertainty in model outputs. The importance of
such a human-in-the-loop workflow has been expressed
by~\textcite{pike_science_2009} in the following quote: ``it is through the interactive
manipulation of a visual interface---the analytic discourse---that
knowledge is constructed, tested, refined and shared.''

Although the mathematical and computational tools we will employ in
this project are generalisable to any modelling workflow, we will
focus on specific application domains where (a) the model is (computationally)
complex, (b) uncertainty is significant and (c) decisions are
time-sensitive. Specifically, we will build interactive interfaces for
 storm-surge and tsunami flood modelling. By working with domain experts in these fields and
building new computational tools and immersive visualisation
environments, we will gather concrete qualitative data on the
usefulness of a dynamic, interactive, exploratory modelling workflows.

The mathematical component of this project will be based on new
mathematical developments combining sparse grids and uncertainty
quantification. One of the main difficulties in current models is the
high dimensional spaces involved, both in the parameter and
domain spaces. Sparse grids~\parencite{BungartzGriebel2004} are
well-known to reduce the effects of the so called ``curse of
dimensionality", whereby the cost of computation increases
exponentially with the dimension of the model.
We have recently found new ways to incorporate gradient
information and multi-fidelity models into sparse grid
approximations~\parencite{deBaarHarding2015,Jakeman2015,deBaarRDM2015} and we will be
deploying this knowledge in our construction of a suite of surge-tsunami models to simulate
environmental disasters with uncertainty quantification.


Computational decision support systems for disaster management
have existed for many decades~\parencite{wallace_decision_1985}, and
recent advances have been made both in incorporating
uncertainty~\parencite{thompson_social_2014-1,neale_navigating_2015}
and providing real-time output~\parencite{yu_support_2006}.
% In these
%(and other) contexts, immersive environments, with multiple large
%screens, rich audiovisual feedback and multi-modal interaction, have
%been used to aid exploration and understanding of simulation
%results~\parencite{colella_participatory_2000,lui_supporting_2014}.
However, existing systems generally only deal with ``known unknowns''
from simulation forecasts. That is, they
rely on specific, apriori knowledge of the nature and
characteristics of the uncertainties involved, and a way to encode and propagate
these uncertainty through the model. This sometimes
falls short in the real world, where the occurrence of ``unknown
unknowns'' may require unforeseen constraints or new knowledge to be
incorporated into a model, or its estimates of uncertainty, in real-time and its results communicated rapidly to decision makers.
We deal with this challenge by 1) building modelling suites which provide rapid and {\em time-bound} feedback of results and/or 
uncertainty, 2) understanding  specific interaction scenarios involved in decision making with expert modelling support and the 
time constraints that such scenarios place on software systems, and 3) deploying
recent developments in programming-language systems, known as ``live
programming'', into the context of software development, scientific modelling and
simulation. 




%hjg
%Live programming is a term which has been used to denote
%systems which support the direct intervention of the programmer in a
%program's run-time state. It can be thought of as an extreme version of
%just-in-time (JIT) programming where there is a direct correlation
%between a program's representation and its execution. An example of
%this is the visual language Self~\parencite{ungar_self_1987} where
%each visual element in the language is a directly programmable object.
%As the ambition of LP systems has grown, these systems and languages
%themselves have evolved to have access to clock-time and other
%computer-system properties at a language level in order to, for
%example, create, modify and interact with music and cyberphysical devices
%in real time. 
%Such an approach has been 
%termed ``Programming with Time"~\parencite{GARDNER AND SORENSEN, 2010}



%HJG
%Examples of the latter include the
%Impromptu~\parencite{sorensen_impromptu_2005} and
%Extempore~\parencite{sorensen_extempore} languages which one of the
%authors of this proposal has been working on and off with over several
%years.
%In addition to this live programming approach, we will use sparse grid
%and reduced basis 
%methods to allow sophisticated approximations to complex models with
%high-dimensional parameter spaces. The use of such methods will allow
%us to perform parameter space exploration more efficiently, resulting
%in an interactive feedback loop even with complex and computationally
%expensive models. The live programming tools provide the
%\emph{capability} to modify the simulation on-the-fly, whilst sparse grids
%and reduced basis models shorten the feedback latency to make this 
%capability \emph{useful}.

This project will provide the following discoveries and benefits:
\begin{enumerate}

\item a suite of interactive, real-time modelling tools for surge-tsunami flood disasters which combine
full simulations with coarse-grid, rough simulations to quantify
uncertainty, optimised for human exploration

\item interactive information visualisations of simulation predictions
together with uncertainties

% Re: the next one, I think we shouldn't push too hard on the "acting
% in the control room while the disaster is happening" angle, since it
% does open us up to a lot of "why would you want to do that"
% sentiment, and the stuff we're building is just as useful in
% pre-planning or dress-rehearsals
% Brendan: I think saying something about robustness would be nice 
% though, especially as we have some ecperience in this area.

% \item software support for running these simulations in unpredictable
% and error-prone environments

\item a live software engineering approach to the development, deployment 
and optimisation of these interactive software systems

\item fundamental understanding of the nature of group interaction scenarios involved in decision making 
for environmental disasters with expert modelling support, and the 
time constraints that such scenarios place impose software systems.

\end{enumerate}

This research is significant because it aims to unlock the power of
sophisticated
computational simulation for interactive use. Although we concentrate
our research on case studies in disaster response, the
ultimate potential of this work is to eventually empower domain
experts from a broad range of areas to use the high-performance
computing power which is now available to them. We envision a future
where performing a complex flood model or disaster simulation is as
interactive and \emph{alive} as flicking through photos on an iPad.

%rather than a tedious ``queue up the job, come back a week later to
%pore over a dump of the results'' process. This benefits not only
%experts already working in that field, but also allows new stakeholders
%(or even schoolchildren) to understand the behaviour of such systems
%through interactive play rather than dry textbooks.

%TODO mention FOR

\subsubsection*{RESEARCH PROJECT}
% - Describe how the research is significant and how it addresses an important problem
% - Describe how the Proposal meets the objectives of the Discovery Projects scheme
% - Describe how the anticipated outcomes will advance the knowledge base of the discipline and
%   how the Proposal aims and concepts are novel and innovative
% - Outline the conceptual framework, design and methods and demonstrate that these are
%   adequately developed, well integrated and appropriate to the aims of the Proposal. Include
%   research plans and proposed timelines
% - Detail what new methodologies or technologies will be developed in the course of the research
%   and how they will advance knowledge
% - Outline the feasibility of the project, in terms of design, budget and proposed timeline
% - If the rationale for some of the Proposal rests upon manuscripts that are still in the process of
%   being published, or on results of work that may not be available to assessors, include a
%   summary of the relevant work
% - Describe the expected outcomes and likely impact of the proposed research
% - Describe how the Proposal might result in national or international economic, commercial,
%   environmental, social and/or cultural benefits
% - If the research has been nominated as focussing upon a topic or outcome that falls within one
%   of the Science and Research Priorities, describe the potential for the project to contribute to the associated Priority Goals.

The January 2011 Brisbane River floods in south-east Queensland cost
32 lives and caused 2.5~billion dollars worth of
damage~\parencite{vandenhonert_2011_2011}. In the days leading up to
these events, a key issue facing authorities was incorporating their
\textbf{uncertainty} about the preceding fortnight's rainfall into the
modelling. In their report on the causes, impacts and implications of
the floods, \textcite[p1170]{vandenhonert_2011_2011} conclude:
\blockquote{whilst the dam operators were acting in accordance with
  the operations manual for the dam, their modelling did not take
  account of forecast rainfall in determining the predicted dam water
  level, and this resulted in a sub-optimal water release strategy.
  Employing tools for decision making under uncertainty would have
  resulted in a different water release strategy.} At the other end of
the environmental spectrum, bushfire model predictions are similarly
\textquote[\cite{alexander_limitations_2013} p375]{fraught with
  uncertainty}. As Australia's climate changes and extreme weather
events become more common, there is a significant need for better ways to
glean timely insights from modelling in the presence of uncertainty.

Computational modelling/simulation is an invaluable tool in these
scenarios, allowing both domain scientists and  emergency services personnel to explore 
what might happen
under various scenarios. Some of the inputs to the model may be well
known, while others are known only approximately, and others still may
only be guessed at.

More formally, we have a mathematical model $M_{\mathbf{p}}$
parameterised by $\mathbf{p}\in\mathcal{P}$. For example, $M_{\mathbf{p}}$ may
be a parameterised partial differential equation (PPDE) in a storm
surge model. For each $\mathbf{p}$ we suppose the model is
well-defined and there exists a unique function
\begin{equation}
  \label{eq:1}
  u_{\mathbf{p}}(\mathbf{x})\, \quad \mathbf{x}\in\Omega
\end{equation}
which is a solution to the model problem, that is $M_{\mathbf{p}}(u_{\mathbf{p}})=0$.

The goal of the scientist is to better understand the relationship
between $\mathbf{p}$ and some lower-dimensional quantity of interest
$Q(u_{\mathbf{p}})$, or to find the parameter choice $\mathbf{p}$
which optimises $Q$. %over all $\mathbf{x}$.
This high-level description of the ``model selection/optimisation''
problem glosses over some nuances, but this general workflow (shown
graphically in Figure~\ref{fig:general-fb-loop}) lies at the heart of
a great deal of modern science.

\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{figures/general-fb-loop.pdf}
  \caption{The human-in-the-loop modelling workflow. A scientist
    selects an initial parameter $\mathbf{p_0}$ for their model,
    examines the model output $Q(u_{\mathbf{p}})$, and either accepts
    the output of the model or re-runs the model with a different
    choice of the parameter $\mathbf{p_1}$.}
  \label{fig:general-fb-loop}
\end{figure}

There are many ways of finding the optimal $\mathbf{p}$, from trial
and error or expert judgements through to fully automated algorithmic
optimization procedures. Often there are ways to optimise $\mathbf{p}$
algorithmically, although this usually introduces new parameters (the
arguments of the function being optimised) which must be selected by the
scientist. As a result, this feedback loop will often require many
iterations, with a human scientist in-the-loop, evaluating the results
of the model (possibly through visualising the model output) and
choosing a parameter update $\Delta\mathbf{p}$ at each step (see
Figure~\ref{fig:unrolled-fb-loop}). Each step through this loop
provides feedback to the scientist about the response of the system to
a particular value of $\mathbf{p}$, for example the maximum storm
surge level under a particular rainfall scenario. Through the process
of trying different model parameterisations, the scientist is able to
build up an understanding of the \emph{general} relationship between
$\mathbf{p}$ and $Q(u_{\mathbf{p}})$, including the areas of the
parameter space $\mathcal{P}$ which have the greatest influence on the
result. This is especially important if the model inputs are not known
with certainty---since the scientist is better able to know which
types of uncertainty have the greatest impact on the certainty of the
results.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/unrolled-fb-loop.pdf}
  \caption{If the modelling \& post-processing/visualisation steps can
    be performed sufficiently quickly, then the scientist can explore
    the $\mathbf{p} \rightarrow Q(u_{\mathbf{p}})$ relationship
    \emph{interactively}, with the all the associated benefits for
    exploratory analysis.}
  \label{fig:unrolled-fb-loop}
\end{figure}

From a workflow perspective, the productivity of the scientist is
proportional to the rate at which they can explore the
$\mathbf{p} \rightarrow Q(u_{\mathbf{p}})$ relationship. Any latency
improvements in this feedback loop will translate into productivity
gains~\parencite{liu_effects_2014}.

If the model parameters $\mathbf{p}$ and inputs $\mathbf{x}$ are known
precisely and the quantity $Q(u_{\mathbf{p}})$ is cheap to calculate
and easy to interpret, then the task is simple: provide the scientist
with an interface for manipulating $\mathbf{p}$ and set them loose.
However, for real-world models (such as those used in flood/storm
surge/bushfire modelling) this is often not the case, there are \textbf{three
primary challenges}:
\begin{enumerate}
\item \emph{The model may not provide a way to express uncertainty in
    the inputs}. Many models do not provide methods for including
  uncertainty information in their inputs, as was the case in the
  Lockyer valley example.
\item \emph{The quantity $Q(u_{\mathbf{p}})$ may not be cheap to
    calculate} (as shown in Figure~\ref{fig:long-fb-loop.pdf}). Many
  sophisticated models require non-trivial computing resources (e.g.
  supercomputers) to evaluate. These compute resources may be
  difficult to secure, with jobs having to wait in a queue, and may
  take a long time to compute even when the resources are available.
  This is especially problematic in a disaster response scenario,
  where an approximately correct answer provided in a short time is
  significantly more useful than a perfect answer provided after it is
  too late to act on.
\item \emph{The quantity $Q(u_{\mathbf{p}})$ may not be easy to
    interpret}. This may be because of technical reasons, such as a
  complex problem domain where coming up with a meaningful loss
  function is difficult, or may be due to ethical reasons---how to
  balance the predicted cost to private property vs damage to the
  natural environment. Finally, this is a visualisation problem---
  the mapping $\mathbf{p} \rightarrow Q(u_{\mathbf{p}})$ may be high-dimensional,
  and presenting that to the scientist (especially with uncertainty
  information) may not be straightforward.
\end{enumerate}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/long-fb-loop.pdf}
  \caption{If the model is computationally expensive to run, then the
    workflow is dominated by waiting for the model to finish. This
    results in lower productivity---not only due to the time spent
    waiting for the model, but also because of the temporal separation
  between the selection of a new parameter $\mathbf{p}$, and seeing
  its impact on the results of the model.}
  \label{fig:long-fb-loop.pdf}
\end{figure}
By accelerating the feedback loop between $\mathbf{p}$ and
$Q(u_{\mathbf{p}})$, including estimates of uncertainty, we will give
the scientist the ability to interactively \emph{explore} the
connection (and the associated uncertainty) between the different
dimensions of $\mathbf{p}$ and the overall response of the system.
Ultimately, the scientist needs an \textbf{interactive interface} for
gleaning insights from their models in the presence of these
challenges.

Our conceptual framework for dealing with these three challenges
involves using sparse grids and reduced basis models 
combined with live modification
of simulation parameters, dynamic computational resources and
visualisation. This approach allows us to deal with the three primary
challenges mentioned above.
\begin{enumerate}
\item \emph{The model may not provide a way to express uncertainty in
    the inputs}. Using sparse grids and reduced basis models to precompute
  a surrogate model, existing models (which may not
  provide a way to encode uncertainty information) can be used as
  ``black boxes'', and expectation integrals over subsets of the
  parameter space $\mathcal{P}$ (which are important for quantifying
  uncertainty) can be efficiently be estimated from the
  surrogate model $\tilde{u}_{\mathbf{p}}(\mathbf{x})$. This has
  significant benefits over classical Monte Carlo methods when the
  integrand is sufficiently
  smooth~\parencite{JakemanRoberts2013,FranzelinDiehlPfluger2014}.
\item \emph{The quantity $Q(u_{\mathbf{p}})$ may not be cheap to
    calculate}. For many problems the computation of solutions 
  $u_{\mathbf{p}}(\mathbf{x})$ to the model $M_{\mathbf{p}}$ can be done 
  cheaply using the combination technique over the domain
  $\mathbf{x}\in\Omega$, as shown in Figure~\ref{fig:sg-surrogate-model-fb-loop}.
  One may also develop reduced basis models to achieve the same outcome.
\item \emph{The quantity $Q(u_{\mathbf{p}})$ may not be easy to
    interpret}. Since a sparse grid sampling of $\mathbf{p}\in\mathcal{P}$ enables a fast 
  and efficient exploration of parameter space, there is more time for
  visualisation and post-processing in an interactive interface, which
  allowing richer ensembles of visualisation techniques to assist the
  scientist in interpreting the results of the model.
\end{enumerate}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/sg-surrogate-model-fb-loop.pdf}
  \caption{Using the sparse grids and reduced basis models, the computationally
    expensive model calculations can be done ahead-of-time and used to
    construct a surrogate model which can be used to re-claim the
    interactive workflow of Figure~\ref{fig:unrolled-fb-loop}}.
  \label{fig:sg-surrogate-model-fb-loop}
\end{figure}

Impact: new algorithms, software, high performance computer systems
and visualisation techniques for time-bound environmental simulation
with uncertainty. New software tools for the simulation of flood
surges and tsunami. New methodologies for rapid and agile software
development and usability using live programming. New knowledge of
human-in-the-loop requirements for support systems in the context of
environmental disaster management. New algorithms, software, high
performance computer systems and visualisation techniques for
time-bound environmental simulation with uncertainty. New software
tools for the simulation of flood surges and tsunami. New
methodologies for rapid and agile software development and usability
using live programming. New knowledge of human-in-the-loop
requirements for support systems in the context of environmental
disaster management.



% meets objectives of DP

% advances knowledge base

% conceptual framework, design and methods (include timelines)

% new methodologies/technologies (how will they advance knowledge?)

% outcomes/impact

% benefits

% national research priority: environmental change, advanced manufacturing?


% here's more stuff about the Brisbane floods if we want it, but I've
% left it out because the above blockquote really covers it...

% As a result, they chose not to include this information in their
% modelling, which resulted in a decision to delay opening the dam
% until it was too late. Had they been able to incorporate their
% uncertainty about the preceding rainfall, they may have been able to
% see the worst-case scenario more clearly and avert the disaster:






% Decision-making in the face of natural disasters is a complex and
% difficult task, and the solution is not to point the finger at poor
% decisions made in the past but to find better ways to incorporate .
% More generally, uncertainty quantification (UQ) is an active research
% topic~\parencite{le_maitre_introduction_2010}. Various frameworks for
% identifying and representing uncertainty in modelling have been
% proposed \parencite[for
% example]{neale_navigating_2015,maceachren_visual_2015,bonneau_overview_2014}\todo{add
%   more refs here}, and incorporates challenges from numerical methods,
% scientific simulation, distributed computing, information
% visualisation and human-computer interaction.

% UQ techniques for representing and understanding modelling in the
% presence of uncertainty have been
% employed successfully~\parencite{thompson_uncertainty_2011}. Most approaches
% to this problem rely on \emph{richer models}---incorporating the
% uncertainty information into the model, either as a probability distribution or some
% other fuzzy number representation~\parencite{guyonnet_hybrid_2003}. These
% approaches sometimes rely on alterations/additions to the models themselves
% \parencite[e.g. adjoint models in][]{errico_what_1997}, and sometimes
% through non-invasive techniques such as Monte Carlo
% sampling~\parencite{roy_comprehensive_2011,nguyen_non_2015}. While
% these techniques to allow for the representation and progagation of
% uncertainty information through a model, they often have the effect of
% making the model more computationally expensive allowing fewer model
% runs, and fewer model alternatives to be considered in a given period
% of time.

% Our aim in this project is to take the opposite \todo{too
%   strong/stark?} approach: using sparse grid (SG) approximations and
% some clever mathematical front-loading of the computational work
% required, we will make the evaluation of a given scenario under the
% model \emph{cheaper}, allowing interactive parameter-space and
% uncertainty exploration even in complex models.



% There are two opposite, and equally unhelpful, ways of dealing with
% uncertainty in modelling. The first is to ignore it, and assume that
% the inputs to our models are known exactly. While this approach can be
% (and indeed is) used when the uncertainties involved are small,
% increasingly complex models means increasingly complex interactions
% between the different parameters, and so it very difficult to say
% which uncertainties are ``small enough'' to ignore. The opposite
% approach is to \emph{ignore} input data about which there is too much
% uncertainty. While this is a more conservative approach, it too can
% lead to real problems, as in the 2011 Brisbane floods.




The mathematical component of this project will be based on new
mathematical developments combining sparse grids~\parencite{BungartzGriebel2004}, 
reduced basis methods~\parencite{LiebermanEtal2010,Peherstorfer2013,ChenSchwab2015}
and uncertainty quantification. 
One of the main difficulties in the study of current scientific models is
the high dimensional spaces involved, often both in the parameter and
domain spaces, $\mathcal{P}$ and $\Omega$ respecitvely. 
This is a significant barrier for the timely evaluation of models due to the 
``curse of dimensionality'' in which the cost scales exponentially with dimension.
By using sparse grids and reduced basis models we will be able to compute
surrogates of the full problem which are lower dimensional and thus cheaper  
to compute whilst maintaining a high order of accuracy.
For example, a general approach would be to find a lower dimensional manifold 
of the parameter space $\mathcal{Q}\subset\mathcal{P}$ over which the 
model is most sensitive using reduced basis methods.
A sparse grid surrogate of the model over $\mathcal{Q}$ can then be 
computed in an offline phase such that in an online phase model solutions
can be efficiently estimated using the surrogate.
For many problems sparse grids can also be used over $\Omega$ when  
computing solutions to the full model. 
This will significantly speed up the computation of a reduced basis.
We will compute sparse grid solutions via the
``combination technique''~\parencite{Griebel1990}. 
Figure~\ref{fig:sparse_grids} depicts the combination technique, the
equivalent sparse grid and the corresponding full grid. 

\begin{figure}
  \centering
  \input{figures/sparse_grid_fig.tex}  
  \caption{Combination grids on the left (with coefficients, marked with
    a blue plus for $+1$ and red minus for $-1$), sparse grid in the
    middle, full grid on the right.}
  \label{fig:sparse_grids}
\end{figure}

Propogating uncertainty in scientific models which are high dimensional 
and/or expensive to compute is a significant challenge.
For models which are not stochastic in nature, and thus have no means
of directly quantifying uncertainty, one must typically estimate statistical 
moments via quadrature rules. Fo high dimensional problems 
it has been shown that sparse grids can estimate these moments faster 
and more accurately than traditional Monte Carlo methods when the 
probability density functions are sufficiently smooth~\parencite{JakemanRoberts2013,FranzelinDiehlPfluger2014}.
Despite the advantages of reduced basis methods, the manifold $\mathcal{Q}$
is still sufficiently high dimensional that the quantification of uncertainty
is a challenge. As such we will continue to develop numerical methods for 
uncertainty quantification through the study of high dimensional approximation.
Replacing the full model with a reduced basis model also adds additional 
uncertainties. One needs to have some idea what the model outcomes may be 
for parameters not lying on the lower dimensional manifold $\mathcal{Q}$.
By using ensemble methods and gradient enhanced 
approximation~\parencite{deBaarHarding2015,Jakeman2015} we will
propogate these uncertainties through the reduced basis model as well
to provide a complete picture of uncertainties in the full model problem.
Incorporating ideas from Kriging, a feature of which is having 
confidence intervals over an interpolant, with sparse grid interpolation
we will be able to express uncertainties explicitly in the surrogate model.
%Recent advances in multi-fidelity methods will also be a useful tool in not
%only in improving the quality of the surrogate but also in estimating uncertainty.

A reliance on high performance computing for the evaluation of scientific 
models provides additional challenges to the technical side of the project. 
Specifically, our algorithms must be highly scalable and robust to erros/faults.
There have been many recent developments in both highly scalable algorithms 
for the sparse grid combination technique by CI Strazdins.
Additionally it has been shown that such computations can be made robust~\parencite{HardingHLS2015,AliEtal2015}.
By leveraging and continuing to develop these algorithms we can ensure that
the components of our software system that require high performance 
computing resources will be both scalable and robust.

In this project we will leverage on-demand compute resources, such as
the Amazon AWS cloud~\parencite{amazon_aws} and the National Compute
Infrastructure NCI Cloud~\parencite{nci_cloud}. Using these cloud
services will further improve the project's ability to deliver timely
results in high-pressure and time-critical decisionmaking scenarios.

% TODO Need to say something about the exemplar/case studies here.

% try to chunk the interactions and thought processes involved in
% real-time decisionmaking

% GOMS model for co-operative scenarios where someone wants
% information, and someone is trying to present it to them

% it's about chunking complex tasks c.f. beginning-middle-end

% the goal: quantitative requirements on the time bounds, which has
% implications for the design of the systems

% interface should be aware of time, just like Extempore. so it's a
% contribution to human-in-the-loop programming

% Why is interactivity important? Because it takes the abstract
% connection between $\mathbf{p}$ and $Q(u_{\mathbf{p}})$ and makes it a malleable,
% responsive, tactile one. What happens when I change $\mathbf{p}$ a little bit
% in this direction? What about in \emph{that} one? How about \emph{both
%   together?}

\subsubsection*{ROLE OF PERSONNEL}
% - Summarise the role, responsibilities and contributions of each Chief Investigator (CI) and Partner Investigator (PI)
% - Summarise the roles and levels of involvement of other Participants, for example, technical staff, Research Associates and other personnel
% - Describe how each Participant will ensure they have the ‘time and capacity’ to undertake the proposed research, taking into account any other grants or roles that they hold.

%The personnel on this grant cover the key research areas discussed in
%the \emph{Research Project} section of this application:
%
%\begin{itemize}
%\item \textbf{Steve Roberts}: uncertainty quantification, sparse grids, flood
%  modelling
%\item \textbf{Markus Hegland}: sparse grids
%\item \textbf{Peter Strazdins}: sparse grids, distributed computing
%\item \textbf{Henry Gardner}: HCI, interfaces
%\end{itemize}

% Brendan: to some extent I have copied/reworked this section from the linkage project with Fujitsu

The personnel involved in this project will be the CIs from the two
ANU colleges, two post-doctoral fellows and four PhD
students.

CI Gardner is responsible for the overall project and will lead the
interactive and visual interface components. The distributed computing
and mathematical component is lead by CI Roberts. One post-doctoral
fellow, to be situated in the Research School of Computer Science
(RSCS), will develop large portions of the live programming tools and
software interface which will interact with the scientific models. The
second post-doctoral fellow, to be situated in the Mathematical
Sciences Institute (MSI), will develop the numerical methods for
computing sparse grid surrogates and reduced basis models which are
able to efficiently propagate uncertainties. 

The four PhD students will be split between the two departments---two
in RSCS and two in MSI, focusing on specific aspects of the project.
In RSCS, these will be the dynamic distributed computing
infrastructure and the development of live interfaces for data
visualisation. In the MSI, these will be the uncertainty
quantification and the development of numerical methods for the sparse
grid technique.

\subsubsection*{RESEARCH ENVIRONMENT}
% - Outline the adequacy and opportunities within the Research Environment in your relevant department, school or research group, and the extent to which it will provide for knowledge growth, innovation, collaboration, mentoring and student training
% - Describe the existing, or developing, research environment within the Administering Organisation and collaborating Organisation(s) which will enable this Project
% - Describe how the Project aligns with the Administering Organisation’s research plans and strategies.

As the home institution of the Extempore live programming environment, 


It seems we need to address three issues: 

\begin{enumerate}
\item the environment within the department/school/research group
\item the environment between the groups---perhaps between the MSI
  group and the CS group, (and also CSIRO?)
\item how does it align with the ANU's research plans and strategies
\end{enumerate}

Ideas\ldots

Numerical methods and applied mathematics---we're kicking goals.

Collaboration across maths and compsci, have track record through e.g.
the Fujitsu grant

CSIRO/ADFA/BOM(? although we haven't mentioned them thus far) for more
fire and flood stuff

\subsubsection*{COMMUNICATION OF RESULTS}
% - Outline plans for communicating the research results to other researchers and the broader community, including but not limited to scholarly and public communication and dissemination.

% Brendan: to some extent I have copied/reworked this section from the
% linkage project with Fujitsu, perhaps someone from computer science
% could add some good comp-sci journals to this list that would be
% relevant to the live-programming and interactive visualisation sides
% of the project. Possibly also mention something about making
% software available, open-source???

% TODO brendan can you maybe put a couple of journals in here?
We will communicate the results of this project by publishing in
top-ranked journals and conferences. Advances in numerical methods
will be communicated in scholarly journals in numerical analysis and
high-performance computing including the journals “SIAM Journal of
Scientific Computing” and “Parallel Computing”, and national and
international conferences like the “SIAM Conference on Computational
Science and Engineering”, the “Computational Techniques and
Applications Conference (CTAC)”, “Supercomputing” and various
international workshops. We will share our results on the
human-in-the-loop workflow in the context of simulations with
uncertainty in human factors conferences and journals and conferences
such as CHI and VL/HCC.

% TODO Steve, Brendan: check that this is ok re: anuga
In addition, the source code contributions of this project will be
released to the public. Both the Extempore live programming system
(\url{https://github.com/digego/extempore}) and the AnuGA shallow
water simulation package
(\url{https://github.com/GeoscienceAustralia/anuga_core}) are
available on GitHub under MIT and GPLv2 licences respectively. We are
committed to accessible and reproducible computational science, and
support these goals by using free software licences and developing our
code in the open on GitHub.

\subsubsection*{MANAGEMENT OF DATA}
% - Outline plans for the management of data produced as a result of the proposed research, including but not limited to storage, access and re-use arrangements.
% - It is not sufficient to state that the organisation has a data management policy. Researchers are encouraged to highlight specific plans for the management of their research data.

% Brendan: does this include distribution of software? or mainly about raw data? Perhaps mention use of the NCI facility and reliance on their backup procedures?



\subsubsection*{REFERENCES}

\vskip -3em
\printbibliography[title=\ ]

\newpage
\setcounter{section}{5} % E1.
\setcounter{subsection}{0}
\subsection{Justification of funding requested from the ARC for the
  duration of the Project}
\label{sec:funding-justification}
% (In no more than 5 A4 pages and within the required format fully
% justify, in terms of need and cost, each budget item requested from
% the ARC. Use the same headings as in the Description column in the
% budget at Part D of this Proposal. )



\newpage
% E2.
\subsection{Details of non-ARC contributions}
\label{sec:non-arc-contributions}
% (In no more than 2 A4 pages and within the required format, provide
% an explanation of how non-ARC contributions will support the
% proposed Project. Use the same headings as in the Description column
% in the budget at Part D of this Proposal. )


\newpage
\setcounter{section}{6}
\setcounter{subsection}{11} % F12. 
\subsection{Research Opportunity and Performance Evidence (ROPE)\\
  Recent significant research outputs and ARC grants (since 2005)}
\label{sec:recent-significant-outputs}
% (Please attach a PDF with a list of your recent significant research
% outputs and ARC grants most relevant to this Proposal (20 pages
% maximum). Please refer to the Instructions to Applicants for the
% required content and formatting.)



\newpage
% F13. 
\subsection{Research Opportunity and Performance Evidence (ROPE)\\
  Ten career-best research outputs}
\label{sec:ten-best-outputs}
% (Please attach a PDF with a list of your 10 career-best research
% outputs, with a brief paragraph for each output explaining its
% significance (5 pages maximum). Please refer to the Instructions to
% Applicants for the required content and formatting.)



\newpage
\setcounter{section}{7} % G1. 
\setcounter{subsection}{0}
\subsection{Research support from sources other than the ARC}
\label{sec:other-research-support}
% (For all participants on this Proposal, please provide details of
% research funding from sources other than the ARC (in Australia and
% overseas) for the years 2014 to 2018 inclusive. That is, list all
% Projects/Proposals/Fellowships awarded or requests submitted for
% funding involving the Proposal participants. Please refer to the
% Instructions to Applicants for submission requirements.)

% \section{Conclusion}
% \label{sec:conclusion}

% Please give us all the money, and we'll do amazing science.

\appendix
\newpage

\todo[inline]{Ok, from here on it's the wild west. Caveat lector.}

\textbf{Ben notes}: uncertainty and risk in decisionmaking - it's a
real problem that bites

BNE floods were in part a human factors problem - they decided to make
decision based on ignorance rather than uncertainty

fire: The myriad of information collected from diverse sources had a broad
range of reliability---\parencite{cruz_anatomy_2012}

\blockquote[\parencite{alexander_limitations_2013}]{Given the nonlinear
  dynamics of free-burning wildland fires (Sullivan 2009d), model
  output may be highly sensitive to a particular parameter over one
  range of values and quite insensitive to that same parameter over a
  different value range (Albini 1976a, Cruz et al. 2006)}

but one other limitation is the modelling is just provided - limited
ability to assess different scenarios other than the ones presented,
and therefore unable to do the helpful ``relative/anchoring'' natural
decisionmaking approach which is what actually happens in these
scenarios

\blockquote[\parencite{vandenhonert_2011_2011} p1158]{In the aftermath of
  the January 2011 flood, a senior Wivenhoe engineer has stated before
  the Commission of Inquiry [17] that uncertainty in BoM rainfall
  forecasts and the paucity of rain gauges in the catchment
  immediately above the Wivenhoe Dam led them to conclude that the
  precipitation forecasts were not sufficiently reliable to form the
  basis for operational decision making.}

naturalistic decisionmaking\parencite{lipshitz_taking_2001}

Can we make the case that the better approach to the uncertainty thing
is the constructivist one? need some refs I reckon


uncertainty and complexity are present and irreducible. human factors!

human in the loop (or something similar) is generally accepted

but all the papers making a big deal of that gloss over the technical
challenges of this + complex models

\textbf{Notes from Henry}:
The proposal needs to document the methodology to be followed. I think
that this could include:

\begin{enumerate}
\item construction of a virtual command room
\item construction of a prototype suite of modelling software
\item interviews with domain experts from Geosciences, Flood
  management, RFT
  \begin{enumerate}
  \item from these interviews, develop principal distraction scenarios
    to be tested in the virtual command room
  \item build a form of cooperative game that would be played by a
    group of test participants
  \end{enumerate}
\item development and testing of interactive visualistions
\end{enumerate}

What this proposal does NOT include is advice from AI agents. That can
be left to others in collaboration with us.

At present, the version of the proposal that I have seen has a focus
on flood modelling. I wonder if we could better position it in the
realms of storm surges. Important for the future. Links to bathymetry
and tsunamis etc. What do people think?

% overview of dynamic decision support
% systems\parencite{gonzalez_decision_2005}

% mceachern on the current SoA in visual analytics and
% uncertainty\parencite{maceachren_visual_2015}

% \textquote[\parencite{sprague_framework_1980}]{interactive computer
%   based systems, which help decision makers utilize data and models to
%   solve unstructured problems.}

\textcite{zack_role_2007} describes four classes (well, $2\times2$ really) of uncertainty:
\begin{itemize}
\item Uncertainty: not having enough information;
\item Complexity: having more information than one can
\item Ambiguity: not having a conceptual framework for interpreting
  information;
\item Equivocality: having several competing or contradictory
  conceptual frameworks.
\end{itemize}

In addition, recent developments in sensor networks and pervasive
monitoring technologies have increased our ability to gather data from
the world. Streaming data analysis is an active area of research, and
in an uncertainty quantification context streaming data provides the
opportunity to be continuously estimating model uncertainties and
updating our models in response to new data coming in.

This project asks the question: \textbf{can streaming data be used to
  update and model uncertainties and provide intelligible insights in
  time-critical decisionmaking scenarios?}

One example scenario: sensors detect an earthquake in the Pacific Ocean,
and the countdown timer begins---what is the likelihood of a
destructive tsunami, where will it impact, and does an evacuation
order need to be issued? Some initial modelling has already been done,
but new readings are coming in regularly from sensor buoys throughout
the ocean. How can this new (and potentially noisy) data be used to
update the model, and how does this affect the predicted impact? How
does uncertainty in the buoy readings affect our confidence in the
model?

\todo[inline]{What follows is from a draft which Henry and Ben started
to put together last year, and there is some material here which could
be used to fill out the ``building the interfaces'' part of the
``Research Project'' section of this application. But it won't
necessarily make sense as-is.}

\subsubsection*{AIMS AND BACKGROUND}
\label{SWfMS-grant}
\paragraph*{Aims}\mbox{}\\

The modern world appears to be saturated with computing power.
Contemporary smart phones outperform the original Cray~1
``supercomputer'' by three orders of magnitude. Accessing contemporary
high-performance computing (HPC) resources has never been easier and,
when combined with machine learning\parencite{Hastie2009}, and big
data\parencite{Manyika2011}, modern supercomputing performance has the
potential to provide answers to deep questions in science, business
and the humanities.

Modern comodity computing devices are spectacularly \textbf{visual}
and \textbf{interactive}, enabling users to slide, tap, watch, speak
and listen with real-time feedback. It is this combination of
interactivity with power which makes mobile and personal computing
useful, usable and an agent of change.

But the closer one comes to true high-performance computing, the less
interactive computing systems become. Due to a combination of
technical and social factors, high-performance scientific computing
(HPSC) is still often performed in a batch \emph{compile-run-analyse}
style: simulation codes are complex both conceptually (i.e.~the
underlying mathematics and physics) and in implementation (distributed
computation, large codebases). Supercomputing resources are expensive
and need to be kept secure.

It is worthwhile imagining some example scenarios that might be
possible if better interactive \textbf{access to} and \textbf{steering
  of} scientific supercomputing applications were to be available:
\begin{enumerate}
\item A computational physicist submits a job to a supercomputer which
  takes 3 weeks to execute. But subsequent monitoring of incremental
  visualisations indicates that something about the simulation does
  not make sense.A check of the, almost 200, input parameters reveals
  that two of these have been incorrectly set. The phyicist cancels
  this run and submits another.

\item That same physicist has a sudden insight into the physical
  system being modelled as a result of watching some of the
  incremental visualisations of the running code. She initiaties a new
  simulation with several changes to the parameter deck before the
  first simulation is complete. Both simulations turn out to be useful
  in understanding the phenomenon under study.

\item Our physicist decides to improve the numerical accuracy of the
  model itself. She quickly modifies one of the functions being called
  by that simulation, one that implements flux-conservation across
  computational elements of the magneto-fluid being modelled. She then
  tests her new function on the active program by branching the
  simulation and swapping the function call on active data. Through a
  comparison of the two active simulations, she is able to estimate
  what the effects of her new numerical method will be on the
  accumulation of errors in the simulation data. She lets both
  simulations run to completion, and in her subsequent data analysis,
  she is able to use her error estimates to adjust and compare
  sumulations using both methods.

\item Performance diagnostics reveal that a multicore simulation is
  very unbalanced in terms of processor allocation. Due to the
  complexity of this simulation, the only way to rebalance resources
  is to rewrite part of the code which maps the geometric grid to the
  processor model. Our physicist is able to rewrite this function and
  to swap the new version in, in real-time. The simulation continues
  seemlessly with enhanced use of resources and finishes faster.

\item A computer scientist is developing a custom visualisation of a
  scientific simulation. Because of the size of the dataset being
  rendered, and because of its distribution across hundreds of
  processors it is impossible to move all of the data. Instead each
  data nodes needs to be ``visited'' and a sample of that data
  transferred for visualisation. This visualisation is planned, tuned
  and debugged on running code and is complete by the time actual the
  simulation is finished.

 % \item A bioinformatics researcher runs a supercomputer simulation which searches and matches millions of generated phylogenic candidates in order to develop evolutionary trees. Rather than 

%A subsequent analysis of the results finds that the stopping
%   criteria were satisfied after only the first hundered such files had
%   been compared.

\item A team of emergency-services personnel are monitoring a
  real-time satellite feed of a bushfire which is hours away from
  threatening homes and lives. A computational simulation of the
  bushfire is being monitored and compared with data but it appears to
  be giving unreliable results. The simulation experts quickly
  initiate several companion simulations having different values of
  fuel loads. The fire-front trajectories of these companion
  simulations neatly bracket those of the main simulation and provide
  an estimate of the confidence that can be ascribed to those
  predicted fire-fronts.


%\item A growing hi-tech manufacturing company has historically relied
%  on managing their supply chain by hand. However, as the company
%  grows and the number of supplier relationships increases this is no
% longer feasible. A purely automated approach generates suboptimal
%  results, because the supply chain involves a complex network of
%  human relationships and hand-shake agreements between the suppliers.
%  The expert team can see subtle problems with the results (and
%  appropriate subtle corrections) as the automated model is running,
%  but are prevented by the software from making these changes until
%  the model has finished running.
\end{enumerate}

All of these scenarios benefit from a tight human-in-the-loop (HIL)
coupling between run-time analysis, visualisation and high-performance
simulation software. The benefits include greater \emph{efficiency}
and greater \textbf{potential for discovery} in scientific workflows.
\textbf{The aim of this project proposal is thus to build
  human-in-the-loop interactivity into high performance scientific
  computing}.

\paragraph*{Background}\mbox{}\\

In 2007, a National Science Foundation (NSF)
workshop\parencite{Gil2007} highlighted the need for dynamic
interactivity as a looming ``grand challenge'' in scientific
computation and data analysis.
% Other challenges included data provenance (where data comes
% from\parencite{McPhillips2009}), reproducibility and shareability of
% complex data processing software pipelines.
In spite of many advances in the field of computational steering it is
clear that challenges remain. A recent (2014) review, ``Dynamic
steering of HPC scientific workflows: A survey''\parencite{Mattoso},
identified six key areas of interaction/steering:
\begin{enumerate}
\item \textbf{monitoring}: the ability to monitor intermediate
  quantities of interest (e.g.~total energy)
\item \textbf{analysis}: the ability to perform analysis of simulation
  quantities (e.g.~viewing a live histogram of the energy
  distribution)
\item \textbf{adaptation}: the ability to adapt/modify the running
  simulation
\item \textbf{notification}: the ability of the system to alert users of the
  need/opportunity to interfere
\item \textbf{interface for interaction}: the interface (either command-based
  or GUI) presented to the user to monitor and adapt the computation
\item \textbf{computing model}: the ability to tailor the types of interaction
  depending on the compute model in use (e.g.~providing full
  monitoring when running locally on a workstation vs only statistical
  sampling when the job is distributed over a cluster)
\end{enumerate}
At the end of their survey, Mattoso et~al.~particularly identify the
\emph{analysis} and \emph{adaptation} branches of their taxonomy as
not being well addressed by the current state of the art.

Elaborating further, in terms of \emph{real-time analysis} the open
challenges are:
\begin{itemize}
\item \textbf{in-situ analysis}: modern terascale compute
  infrastructure is often IO-bound, and HPC applications must minimise
  inter-node communication to achieve efficient use of the hardware.
  As a result, it is often unfeasible to move the data of interest
  back to a central node for processing and analysis, so any such
  analysis must be performed in-situ\parencite{Bennett2012} on the compute
  node itself.
\item \textbf{decision-support tools}: giving a user the
  \emph{ability} to interfere with a running computational process is
  one thing---knowing \emph{when and how} to interfere is another
  thing altogether. Dynamic SWfMS must provide as useful real-time
  feedback to the user (a domain expert) to assist them in deciding
  when and how to interfere. This problem is an example of a more
  general end-user programming (EUP)\parencite{Myers2006} problem---giving
  domain experts the appropriate tools to transfer their domain
  expertise into the computational domain.
\end{itemize}
In the area of \emph{adaptation} the open challenges are:
\begin{itemize}
\item \textbf{dynamic workflow engines}: most of the surveyed systems
  allowed the user to adapt the computation at a high level (e.g.
  add/delete subtasks). However, there was much less support for
  modifying the evolution of specific tasks (e.g.~swapping out
  low-level algorithms and tight-loops in computation). This level of
  adaptivity would allow for even more steering possibilities, but may
  exacerbate the problem of deciding when/how to interfere (although
  support for rolling back interventions would be helpful here).
\item \textbf{parameter slice exploration}: many scientific problems
  involve searching high-dimensional parameter spaces for optimal
  parameter configurations, and many algorithms exist for this task.
  However, there are some problems (e.g.~buisness informatics) where
  cost/loss functions are hard to articulate, and more traditional
  purely numerical optimization approaches tend to overfit or get
  stuck in local maxima. In these situations an expert domain user, if
  presented with appropriate feedback in real-time, may be able to
  recognise these type of convergence problems early on, and perform
  appropriate algorithmic and parameter adaptations to avoid the
  problem.
\end{itemize}


\subsubsection*{RESEARCH PROJECT}

% explore $\rightarrow$ enliven $\rightarrow$ empower

% Want to bring the idea of ``liveness'' to HPC/big data software – allow
% developers to get rapid, incremental update of target data process
% algs ; allow developers to get near-instantaneous feedback on input of
% changes to outputs data via rich visualisations

% Want to leverage domain-specific visual languages to specify these big
% data analysis problems to enable end users a high-level, controlled
% environment c.f. R-type scripting, code, BI configs etc

% Want to leverage rich information visualisations to provide small
% snapshot and overall output understanding by end users

% << Worth a big picture ``concept diagram'' here >>

% The aim of this project is thus to invent a new approach to specifying
% and realizing data-centric software systems using live visual
% programming and data visualisation we break into specific aims:

% - SDL++ for data analytics
% - Live visual programming of data analytics - DSVLs
% - extempore -> live update of GPU/HPC/IR/ML algs etc
% - live visualisations of complex data analytics alg data for feedback,
%   output
% - evaluate effectiveness by apply to several exemplars e.g.~particles,
%   big data analysis, infoVis...

This project will allow domain experts to better harness the power of
computational simulation the in a safe and effective way by creating
tools and interfaces for \emph{interactive} computational simulation
and anaylsis. This involves integrating research and tools from the
domains where such computational analyses are used with insights from
human-computer interaction and software workflows.

Through a series of exemplar case studies in different domains, we
will unlock new ways to perform complex computation analyses through
three steps:

\begin{enumerate}
\item \textbf{explore}: first, by exploring the problem domain (and
  existing tools) of interest in conjunction with domain experts, do
  determine the dimensions of the domain which could most benefit from
  interactive, human-in-the-loop analysis
\item \textbf{enliven}: taking these modelling and simulation codes
  and providing new interfaces for real-time interaction---this is not
  just a matter of superficially wrapping these systems in a slick
  GUI, but requires a restructuring of codes which are designed with
  batch processing in mind to allow for meaningful interaction and
  feedback
\item \textbf{empower}: using a user-centred design approach, we will
  create appropriate interfaces (e.g.~graphical user interfaces) to
  empower these domain experts to unlock their expertise in
  interactive computational simulation
\end{enumerate}

\paragraph*{Research significance}\mbox{}\\


Computational simulation and modelling is ubiquitous in both academia
and industry, and its importance is growing. Large-scale basic physics
relies on computational simulation to probe the subatomic landscape;
the auto industry uses simulation extensively for calculating the
efficiency of new designs; and algorithmic pattern recognition and
recommendation has transformed the way we search (Google), shop
(Amazon), watch (Netflix).

However, performing these analyses increasingly requires specialised
computational skills such as high-performance and distributed
computing. There is a risk that the power of simulation is available
only to a HPC-literate elite, and to those rare experts who assimilate
both the domain knowledge and computing skills to access it, and to
interpret it's results.

\paragraph*{Advancing the knowledge base with innovation}\mbox{}\\

As highlighted by Mattoso et~al.\parencite{Mattoso}, there is a desire for
more dynamic interactivity in scientific workflows, but progress in
that area has been limited. This project will contribute to the
knowledge base by providing these dynamic workflows, allowing us to
answer open questions such as:

\begin{itemize}
\item how can interactivity most effectively be ``added'' to existing
  scientific workflows?
\item how can automated assistance/analysis be provided to the user to
  best guide their interaction?
\item how do other outstanding issues in scientific computation, such
  as data provenance, reproducibility, etc.~impact the interactive
  workflow?
\end{itemize}

\end{document}

% Local Variables:
% TeX-engine: xetex
% End:
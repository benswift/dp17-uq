\subsubsection*{The Modelling Problem}

More formally, suppose that we have a mathematical model
$M_{\mathbf{p}}$ parameterised by the vector
$\mathbf{p}\in\mathcal{P}$. As in our example, $M_{\mathbf{p}}$ may be a
parameterised partial differential equation such as equation~\eqref{eqn:storm}.
For each $\mathbf{p}$ we suppose the model is
well-defined and there exists a unique function
\begin{equation}
  \label{eq:1}
  U_{\mathbf{p}}(\mathbf{x})\, \quad \mathbf{x}\in\mathcal{D}
\end{equation}
which is a solution to the model problem, that is $M_{\mathbf{p}}(U_{\mathbf{p}})=0$.
Here the \emph{domain space} $\mathcal{D}$ represents variables such as 
time and space (such as $\Omega \times T$ from our example) 
and $U_{\mathbf{p}}$ represents scalar or vector fields of interest, 
such as storm surge water level and/or velocity. 

Sometimes the goal of the modeller is to better understand the
relationship between $\mathbf{p}$ and some lower-dimensional quantity
of interest $Q(U_{\mathbf{p}})$, such as the relationship between a
particular cyclone scenario and the maximum storm surge levels at
particular geographical locations. In order to achieve such
understanding, repeated executions of the model (or approximations of the model solution) are required
with an expert analysis of the outputs of each stage and selection of
the next parameter set based on an assessment of those outputs. Such
repeated model executions can also be used to estimate the
\emph{uncertainty} in any given set of predictions of
$Q(U_{\mathbf{p}})$. Through the process of trying different model
parameterisations, the scientist is able to build up an understanding
of the \emph{general} relationship between $\mathbf{p}$ and
$Q(U_{\mathbf{p}})$, including the areas of the parameter space
$\mathcal{P}$ which have the greatest influence on the result and
which types of uncertainty have the greatest impact on the certainty
of the results.






%or to find the parameter choice $\mathbf{p}$
%which optimises $Q$. %over all $\mathbf{x}$.
%This high-level description of the ``model selection/optimisation''
% workflow 
%(shown
%graphically in Figure~\ref{fig:general-fb-loop})
 %lies at the heart of
%a great deal of modern science.
%\begin{figure}
 % \centering
  %\includegraphics[width=.6\textwidth]{figures/general-fb-loop.pdf}
  %\caption{The human-in-the-loop modelling workflow. A scientist
    %selects an initial parameter $\mathbf{p_0}$ for their model,
    %examines the model output $Q(u_{\mathbf{p}})$, and either accepts
    %the output of the model or re-runs the model with a different
    %choice of the parameter $\mathbf{p_1}$.}
  %\label{fig:general-fb-loop}
%\end{figure}
%There are many ways of finding an optimal $\mathbf{p}$, from 
%trial-and-error experimentation to expert judgement through to fully-automated algorithmic
%optimization procedures. Often there are ways to optimise $\mathbf{p}$
%algorithmically, although these methods often introduce new parameters (the
%arguments of the function being optimised) which must be selected by the
%scientist. 
%As a result, this feedback loop will often require many
%iterations, with a scientist-in-the-loop, evaluating the results
%of the model (possibly through visualising the model output) and
%choosing a parameter update $\Delta\mathbf{p}$ at each step.
% (see
%Figure~\ref{fig:unrolled-fb-loop}). 
%Each step through this loop
%provides feedback to the scientist about the response of the system to
%a particular value of $\mathbf{p}$ --  for example, the maximum storm
%surge level under a particular rainfall scenario. 

%\begin{figure}
% \centering
%  \includegraphics[width=\textwidth]{figures/unrolled-fb-loop.pdf}
%  \caption{If the modelling \& post-processing/visualisation steps can
%    be performed sufficiently quickly, then the scientist can explore
%    the $\mathbf{p} \rightarrow Q(u_{\mathbf{p}})$ relationship
%    \emph{interactively}, with the all the associated benefits for
%    exploratory analysis.}
%  \label{fig:unrolled-fb-loop}
%\end{figure}

From a workflow perspective, the productivity of the
modeller/scientist is proportional to the rate at which they can
explore the $\mathbf{p} \rightarrow Q(U_{\mathbf{p}})$ relationship.
Any latency improvements in this feedback loop will translate into
productivity gains~\parencite{liuEffects2014}. If the model
parameters $\mathbf{p}$ and inputs $\mathbf{x}$ are known precisely
and the quantity $Q(U_{\mathbf{p}})$ is cheap to calculate and easy to
interpret, then the task is simple: provide the scientist with an
interface for manipulating $\mathbf{p}$ and set them loose. However,
for real-world models (such as those used in flood/storm
surge/bushfire modelling) this is often not the case. There are
\textbf{three primary challenges}:
\begin{enumerate}
\item \emph{The model may not provide a way to express uncertainty in
    the inputs}. Many models do not provide methods for including
  uncertainty information in their inputs, as was the case in the 2011
  Brisbane River flood example.
\item \emph{The quantity $Q(U_{\mathbf{p}})$ may not be cheap to
    calculate}. Many
  sophisticated models require non-trivial computing resources 
  to evaluate. These compute resources may be
  difficult to secure, with submitted jobs having to wait in a queue, 
  and may
  take a long time to compute even when the resources are available.
  This is especially problematic in a disaster-response scenario,
  where an approximately correct answer provided in a short time is
  significantly more useful than a perfect answer provided after it is
  too late to act on.
\item \emph{The quantity $Q(U_{\mathbf{p}})$ together with estimates
    of its uncertainty may not be easy to interpret}. Oftentimes this
  is a visualisation problem--- the mapping
  $\mathbf{p} \rightarrow Q(U_{\mathbf{p}})$ may be high-dimensional,
  and presenting that to a decision-maker, particularly when combining
  it with its uncertainty estimates, may not be straightforward.
\end{enumerate}
%\begin{figure}
 % \centering
  %\includegraphics[width=\textwidth]{figures/long-fb-loop.pdf}
  %\caption{If the model is computationally expensive to run, then the
   % workflow is dominated by waiting for the model to finish. This
    %results in lower productivity---not only due to the time spent
    %waiting for the model, but also because of the temporal separation
  %between the selection of a new parameter $\mathbf{p}$, and seeing
 % its impact on the results of the model.}
 % \label{fig:long-fb-loop.pdf}
%\end{figure}
As described in the following sections, we will address these {three primary challenges} using {\bf sparse grids and reduced basis models}, {\bf live coding} and {\bf interactive visualisation}. Specifically: 
\begin{enumerate}
\item {\bf Sparse grids and reduced basis models} will be used
  to pre-compute a surrogate model, $\mathbf{p} \rightarrow \tilde{U}_{\mathbf{p}}$, of the original
  (expensive) model solution process, $\mathbf{p} \rightarrow {U}_{\mathbf{p}}$.
  The expectation integrals of the original model over subsets of the
  parameter space $\mathcal{P}$ (which are important for quantifying
  uncertainty) can be efficiently estimated from the
  surrogate model. This has
  significant benefits over classical Monte Carlo methods when the
  integrand is sufficiently
  smooth~\parencite{JakemanRoberts2013,FranzelinDiehlPfluger2014}.

\item {\bf Live coding} will be used to build systems which guarantee modelling results within timeframes
relevant to decision making in disaster management scenarios as described below.



%for many problems the computation
%  of solutions $U_{\mathbf{p}}(\mathbf{x})$ to the model
%  $M_{\mathbf{p}}$ can be done cheaply using the combination technique
%  over the domain $\mathbf{x}\in\mathcal{D}$. An alternative is to use
%  proper orthogonal decompositions or greedy algorithms to construct a
%  reduced order model which is cheap to compute. In both
%  cases the initial construction can be somewhat costly but the
%  savings gained later when these surrogates are used extensively for
%  UQ and optimisation significantly outweigh this cost. Additionally,
%  the initial construction can be done in an {\bf offline} phase. For our
 % disaster response scenarios, the concept of `cheapness' will be
 % re-framed as one of `timeliness'. We will build systems which
 % guarantee modelling results within timeframes of
  %relevance to human factor processes in group decision making
  %(occurring in the {\bf online} phase). We will use {\bf live coding} to understand these human
  %constraints and to rapidly prototype our sytems for delivery into time-bound environments.   

\item {\bf Interactive visualisations} will be developed to assist decision makers
in interpreting the results of our models. Since sparse grid sampling of
  $\mathbf{p}\in\mathcal{P}$ enables a fast and efficient exploration
  of parameter space, there is more time for {\bf visualisation and
  post-processing} in the interactive interface.
\end{enumerate}

